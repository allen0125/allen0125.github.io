{"componentChunkName":"component---src-templates-blog-post-js","path":"/use_gbdt/","result":{"data":{"site":{"siteMetadata":{"title":"Allen's Blog"}},"markdownRemark":{"id":"d1a24231-3994-5742-bf92-d66f10e69c19","excerpt":"什么是GBDT？ DT－Decision Tree决策树，GB是Gradient Boosting，是一种学习策略，GBDT的含义就是用Gradient Boosting的策略训练出来的DT模型。模型的结果是一组回归分类树组合(CART Tree Ensemble)： T1…Tk。其中 Tj 学习的是之前 j-…","html":"<h2>什么是GBDT？</h2>\n<p>DT－Decision Tree决策树，GB是Gradient Boosting，是一种学习策略，GBDT的含义就是用Gradient Boosting的策略训练出来的DT模型。模型的结果是一组回归分类树组合(CART Tree Ensemble)： T1…Tk。其中 Tj 学习的是之前 j-1 棵树预测结果的残差，这种思想就像准备考试前的复习，先做一遍习题册，然后把做错的题目挑出来，在做一次，然后把做错的题目挑出来在做一次，经过反复多轮训练，取得最好的成绩。</p>\n<p>而模型最后的输出，是一个样本在各个树中输出的结果的和。</p>\n<h2>GBDT + LR ?</h2>\n<p>单独的使用GBDT模型，容易出现过拟合，在实际应用中往往使用 GBDT＋LR的方式做模型训练，算法更多细节可以参考 [Practical Lessons from Predicting Clicks on Ads at Facebook]。本文只介绍结论性的做法。</p>\n<p>首先根据样本训练出GBDT树，对于每个叶子节点，回溯到根节点都可以得到一组组合特征，所以用叶子节点的标号可以代表一个新的组合特征。</p>\n<p>这部分特征是GBDT生成的组合特征，再结合LR固有的稀疏特征，就组成了 GBDT ＋ LR 模型。生成样本向量阶段，样本首先过GBDT模型，生成组合特征部分的输入向量，再结合固有的稀疏特征向量，组成新的特征向量。</p>\n<p>最后将GBDT特征向量与LR稀疏向量组成一个最终的特征向量，并使用该向量训练LR模型。</p>\n<h2>实战</h2>\n<h3>单独训练 GBDT 查看过拟合现象:</h3>\n<h3>结合 GBDT 和 LR 对比单独使用 GBDT 查看是否还存在过拟合现象？</h3>\n<p><a href=\"https://zhuanlan.zhihu.com/p/30339807\">参考</a></p>","frontmatter":{"title":"理解GBDT及小试牛刀","date":"March 23, 2021","description":"通过记录理解GBDT的过程加深对GBDT的印象，然后在网上找到合适的数据集进行GBDT的实验以及GBDT+LR解决GBDT容易过拟合的实验。"}}},"pageContext":{"slug":"/use_gbdt/","previous":{"fields":{"slug":"/motorcycle-driving-licence/"},"frontmatter":{"title":"驾照增驾D摩托车驾驶资格过程回顾"}},"next":null}}}